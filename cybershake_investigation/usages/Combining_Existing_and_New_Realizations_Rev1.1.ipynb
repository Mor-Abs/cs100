{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code Description:\n",
    "This script performs the following tasks to consolidate and manage IM (Intensity Measure) data \n",
    "from existing and new realizations:\n",
    "\n",
    "1. **Data Consolidation**:\n",
    "   - Combines all IM values from existing and new realizations into a newly generated combined directory.\n",
    "   - Identifies common stations and IM types for each source (fault) across old and new realizations.\n",
    "   - Filters out uncommon stations and IM types to ensure consistency.\n",
    "   ---Note: Due to Nan and integer filteration lines added, different realizations may have different number of stations.\n",
    "      This happened in for subduction faults realizations in v24.9\n",
    "      \n",
    "\n",
    "2. **Median Calculation**:\n",
    "   - Computes the median IM values for each station and IM type from all realizations.\n",
    "   - Saves the computed median values as a CSV file.\n",
    "\n",
    "3. **Source File Management**:\n",
    "   - Copies all \"Source\" files from old and new realizations into the \"Source\" directory in the combined folder.\n",
    "   - Copies `nzvm.cfg` and `vm_params.yaml` files from the old realizations as-is.\n",
    "   - Copies these files from the new realizations with a \"_new\" suffix added to the filenames for differentiation.\n",
    "\n",
    "4. **Summary Reports**:\n",
    "   - Generates summary reports, including an Excel file, that details the number of stations and IMs processed.\n",
    "   - Adds a note file providing context and metadata about the combined data.\n",
    "\n",
    "Author: Morteza\n",
    "Version History:\n",
    "- Version 1.0: August 18, 2024\n",
    "- Version 1.1: December 18, 2024\n",
    "\n",
    "To Do:\n",
    " - in the \"Merge IMs\" section, first load each realization's IM values in a dataarray, then concatenate them \"inner\". This will \n",
    "   ensure that only common stations and IM types are retained across all realizations. Then save csv files.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path and directory works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created the path: \n",
      "/mnt/hypo_data/mab419/cs100/Cybershake_Data/combined_cs100\n"
     ]
    }
   ],
   "source": [
    "file_path = Path.cwd()\n",
    "root_path = str(file_path.parent.parent)\n",
    "\n",
    "versions = [\n",
    "    # \"v21p1\",\n",
    "    \"v21p6\",\n",
    "    \"v21p6p2\",\n",
    "    \"v22p2\",\n",
    "    \"v22p4\",\n",
    "    \"v23p5\",\n",
    "    \"v23p7\",\n",
    "    \"v23p10\",\n",
    "]\n",
    "Comp = \"rotd50\" # \"000\", \"090\", \"geom\", \"rotd100_50\", \"rotd50\", \"ver\"\n",
    "\n",
    "vers_path = {\n",
    "    v: os.path.join(root_path, \"Cybershake_Data\", v) for v in versions\n",
    "}\n",
    "\n",
    "combined_cs_files_path = os.path.join(\n",
    "    root_path, \"Cybershake_Data\", \"combined_cs100\"\n",
    ")\n",
    "\n",
    "log_file_path = combined_cs_files_path\n",
    "log_file = os.path.join(combined_cs_files_path, 'combination_logs.txt')\n",
    "\n",
    "# Check if the directory exists\n",
    "if Path(combined_cs_files_path).exists():\n",
    "    prompt = input(\n",
    "        f\"The following path already exists! Do you want to delete and renew (1) or terminate (2)? \\n{combined_cs_files_path}\\n\"\n",
    "    )\n",
    "    if prompt == \"1\":\n",
    "        # Remove the folder\n",
    "        shutil.rmtree(combined_cs_files_path)\n",
    "        print(f\"Deleted and renewed the path: \\n{combined_cs_files_path}\")\n",
    "        os.makedirs(combined_cs_files_path, exist_ok=False)\n",
    "    elif prompt == \"2\":\n",
    "        print(\"Terminating the process.\")\n",
    "        exit()\n",
    "    else:\n",
    "        print(\"Invalid input. Terminating the process.\")\n",
    "        exit()\n",
    "else:\n",
    "    os.makedirs(combined_cs_files_path, exist_ok=False)\n",
    "    print(f\"Created the path: \\n{combined_cs_files_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/141 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 29/141 [08:01<29:06, 15.60s/it] /tmp/ipykernel_3907238/1150536705.py:98: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cur_im_df = pd.read_csv(\n",
      " 82%|████████▏ | 116/141 [36:41<07:01, 16.85s/it] /tmp/ipykernel_3907238/1150536705.py:98: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cur_im_df = pd.read_csv(\n",
      "100%|██████████| 141/141 [44:07<00:00, 18.78s/it]\n"
     ]
    }
   ],
   "source": [
    "def strip_and_convert(value):\n",
    "    \"\"\"\n",
    "    Strips leading/trailing whitespace or single quotes from a value\n",
    "    and converts it to a float if possible.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return float(str(value).strip(\"'\"))\n",
    "    except ValueError:\n",
    "        return value  # Return as-is if it can't be converted\n",
    "    \n",
    "def log_and_print(log_lines: list, msg: str, print = False):\n",
    "    if print:\n",
    "        print(msg)\n",
    "    log_lines.append(msg)\n",
    "    \n",
    "log_lines = []\n",
    "    \n",
    "im_true_order = [\n",
    "    \"PGA\",\n",
    "    \"PGV\",\n",
    "    \"CAV\",\n",
    "    \"AI\",\n",
    "    \"Ds575\",\n",
    "    \"Ds595\",\n",
    "    \"MMI\",\n",
    "    \"pSA_0.01\",\n",
    "    \"pSA_0.02\",\n",
    "    \"pSA_0.03\",\n",
    "    \"pSA_0.04\",\n",
    "    \"pSA_0.05\",\n",
    "    \"pSA_0.075\",\n",
    "    \"pSA_0.1\",\n",
    "    \"pSA_0.12\",\n",
    "    \"pSA_0.15\",\n",
    "    \"pSA_0.17\",\n",
    "    \"pSA_0.2\",\n",
    "    \"pSA_0.25\",\n",
    "    \"pSA_0.3\",\n",
    "    \"pSA_0.4\",\n",
    "    \"pSA_0.5\",\n",
    "    \"pSA_0.6\",\n",
    "    \"pSA_0.7\",\n",
    "    \"pSA_0.75\",\n",
    "    \"pSA_0.8\",\n",
    "    \"pSA_0.9\",\n",
    "    \"pSA_1.0\",\n",
    "    \"pSA_1.25\",\n",
    "    \"pSA_1.5\",\n",
    "    \"pSA_2.0\",\n",
    "    \"pSA_2.5\",\n",
    "    \"pSA_3.0\",\n",
    "    \"pSA_4.0\",\n",
    "    \"pSA_5.0\",\n",
    "    \"pSA_6.0\",\n",
    "    \"pSA_7.5\",\n",
    "    \"pSA_10.0\",\n",
    "]    \n",
    "\n",
    "vers_faults = {\n",
    "    v: [cur_dir.stem for cur_dir in Path(vers_path[v]).iterdir() if cur_dir.is_dir()]\n",
    "    for v in versions\n",
    "}\n",
    "\n",
    "all_faults = list(set().union(*vers_faults.values()))\n",
    "\n",
    "summary_dat_columns = [col for v in versions for col in (f\"n_rels_{v}\", f\"n_stations_{v}\", f\"n_IMs_{v}\")] + [\"n_rel_combined\", \"n_stations_combined\", \"n_IMs_combined\"]\n",
    "summary_data = pd.DataFrame(index=all_faults, columns=summary_dat_columns)\n",
    "\n",
    "##########################################################################\n",
    "########################### working on IM data ###########################\n",
    "##########################################################################\n",
    "\n",
    "for cur_fault in tqdm(all_faults):\n",
    "    cur_fault_versions = [\n",
    "        v for v in versions if cur_fault in vers_faults[v]\n",
    "    ]\n",
    "    cur_fault_versions.sort()\n",
    "    IM_DATA = {v: None for v in cur_fault_versions}\n",
    "    for v in cur_fault_versions:\n",
    "        # identify existing relaizations\n",
    "        fault_version_realizations = [rel for rel in Path(Path(vers_path[v]) / cur_fault / \"IM\").rglob(\"*REL*.csv\")]\n",
    "        fault_version_realizations.sort()\n",
    "        \n",
    "        # write number of realizations to summary data\n",
    "        summary_data.loc[cur_fault, f\"n_rels_{v}\"] = len(fault_version_realizations)\n",
    "        \n",
    "        REL_DATA = {cur_im_file.stem: None for cur_im_file in fault_version_realizations}\n",
    "        file_counter = 0\n",
    "        for cur_im_file in fault_version_realizations:\n",
    "            # Read the current IM file\n",
    "            with open(cur_im_file, 'r') as f:\n",
    "                header = f.readline().strip().split(',')\n",
    "\n",
    "            converters = {\n",
    "                col: strip_and_convert for col in range(2, len(header))\n",
    "            }\n",
    "\n",
    "            cur_im_df = pd.read_csv(\n",
    "                cur_im_file,\n",
    "                converters=converters\n",
    "            )\n",
    "            \n",
    "                     \n",
    "            cur_im_df.columns = [\n",
    "                (\n",
    "                    col[:5] + col[5:].replace(\"p\", \".\", 1)\n",
    "                    if col.startswith(\"pSA_\") and \"p\" in col[5:]\n",
    "                    else col\n",
    "                )\n",
    "                for col in cur_im_df.columns\n",
    "            ]\n",
    "            \n",
    "            # writing summary of data assuming that all realizations in a version has the same stations and IMs\n",
    "            if file_counter == 0:\n",
    "                summary_data.loc[cur_fault, f\"n_stations_{v}\"] = len(set(cur_im_df['station']))\n",
    "                summary_data.loc[cur_fault, f\"n_IMs_{v}\"] = len(list(set(cur_im_df.columns) - {'station', 'component'}))\n",
    "                file_counter += 1\n",
    "            \n",
    "            # Filter non-valid IMs\n",
    "            temp_df1 = cur_im_df.copy()\n",
    "            temp_df1.iloc[:, 0] = temp_df1.iloc[:, 0].astype(str)\n",
    "            temp_df1.iloc[:, 1] = temp_df1.iloc[:, 1].astype(str)\n",
    "\n",
    "            float_cols = temp_df1.columns[2:]\n",
    "            temp_df1.loc[:, float_cols] = temp_df1.loc[:, float_cols].apply(\n",
    "                pd.to_numeric, errors='coerce'\n",
    "            ).astype(np.float64)\n",
    "\n",
    "            df_float = temp_df1[float_cols]\n",
    "            valid = (~df_float.isna()) & (df_float != 0) & (df_float != 1) & (df_float % 1 != 0) # Exclude 0, 1, nan, and integers\n",
    "            mask = valid.all(axis=1) # check for each row where all its colums are true.\n",
    "            cleaned_df = temp_df1[mask].copy() # remove stations with at least one invalid value\n",
    "            \n",
    "            valid = valid.set_index(temp_df1['station'])\n",
    "            stacked_valid = valid.stack()\n",
    "            unvalid_locations = stacked_valid[~stacked_valid].index.tolist()\n",
    "            if len(unvalid_locations) != 0:\n",
    "                for item in unvalid_locations:\n",
    "                    msg = f\"Warning(Invalid_Value): IM {item[1]} at station {item[0]} for {cur_im_file}  at version {v} is invalid and removed.\"\n",
    "                    log_and_print(log_lines, msg)\n",
    "            \n",
    "            # identifying missing IMs\n",
    "            for col in im_true_order:\n",
    "                if col not in cleaned_df.columns:\n",
    "                    msg = f\"Warning(Missed_IM): The IM {col} does not exist in the {cur_im_file} \"\n",
    "                    log_and_print(log_lines, msg)\n",
    "                    # cleaned_df[col] = np.nan\n",
    "                    \n",
    "            #filtering only the component of interest\n",
    "            filtered_im_df = cleaned_df[[\"station\", \"component\"] + im_true_order]\n",
    "            filtered_im_df = filtered_im_df[filtered_im_df[\"component\"] == Comp].copy()\n",
    "            if filtered_im_df.empty:\n",
    "                msg = f\"Warning(Empty_Filtered_File): {cur_im_file} in version {v} is empty after filtering. Maybe no valid IMs or Component {Comp} found.\"\n",
    "                log_and_print(log_lines, msg)\n",
    "            \n",
    "            # write data to database\n",
    "            REL_DATA[cur_im_file.stem] = filtered_im_df\n",
    "            \n",
    "        IM_DATA[v] = REL_DATA\n",
    "    \n",
    "    # identify common sations for a fault    \n",
    "    common_stations = set.intersection(\n",
    "        *[set(df[\"station\"]) for rel_data in IM_DATA.values() for df in rel_data.values() if df is not None ]\n",
    "    )\n",
    " \n",
    "    if len(common_stations) == 0:\n",
    "        msg = f\"Warning(All_Reals_Common_Station): Realizations of {cur_fault} do not share common stations among diffferent versions. \"\n",
    "        log_and_print(log_lines, msg)\n",
    "    \n",
    "    # filtering only the common stations \n",
    "    common_columns = set(im_true_order)\n",
    "    for v in cur_fault_versions:\n",
    "        for rel in IM_DATA[v].keys():\n",
    "            tempdf = IM_DATA[v][rel]\n",
    "            tempdf = tempdf[tempdf['station'].isin(common_stations)].copy()\n",
    "            if len(tempdf) == 0:\n",
    "                msg = f\"Warning(Single_Real_Common_Station): The realization {rel} of fault {cur_fault} in version {v} lost all its data while combining as it does not share common stations with other realizations.\"\n",
    "                log_and_print(log_lines, msg)\n",
    "            IM_DATA[v][rel] = tempdf\n",
    "    \n",
    "    # identify common IM values \n",
    "    common_columns = set(im_true_order).intersection(\n",
    "        *[set(df.columns) for rel_data in IM_DATA.values() for df in rel_data.values() if df is not None ]\n",
    "    )\n",
    "    common_ims_list = sorted(list(common_columns), key=im_true_order.index)\n",
    "    correct_columns = ['station', 'component'] + common_ims_list       \n",
    "    \n",
    "    # filtering only the common columns\n",
    "    for v in cur_fault_versions:\n",
    "        for rel in IM_DATA[v].keys():\n",
    "            tempdf = IM_DATA[v][rel]\n",
    "            tempdf = tempdf[correct_columns].copy()\n",
    "            IM_DATA[v][rel] = tempdf\n",
    "    \n",
    "    # writing IM to new files\n",
    "    os.makedirs(os.path.join(combined_cs_files_path, cur_fault, 'IM'))\n",
    "    file_counter = 0\n",
    "    for v in cur_fault_versions:\n",
    "        for rel in IM_DATA[v].keys():\n",
    "            file_counter += 1\n",
    "            tempdf = IM_DATA[v][rel]\n",
    "            target_im_file = os.path.join(combined_cs_files_path, cur_fault, 'IM', f\"{cur_fault}_REL{file_counter:02d}.csv\")\n",
    "            tempdf.to_csv(target_im_file, index=False)\n",
    "            msg = f\"Note(Version_Control):  {v}/{rel}  -->  {cur_fault}_REL{file_counter:02d}.csv\"\n",
    "            log_lines.append(msg)\n",
    "            if file_counter == 1:\n",
    "                summary_data.loc[cur_fault, \"n_stations_combined\"] = len(set(tempdf['station']))\n",
    "                summary_data.loc[cur_fault, \"n_IMs_combined\"] = len(list(set(tempdf.columns) - {'station', 'component'}))\n",
    "    \n",
    "    summary_data.loc[cur_fault, \"n_rel_combined\"] = len([rel for v in cur_fault_versions for rel in IM_DATA[v]])\n",
    "    \n",
    "    ###############################################################################\n",
    "    ########################### working on Median files ###########################\n",
    "    ###############################################################################\n",
    "    \n",
    "    # create median file\n",
    "    all_dfs = [\n",
    "        df.set_index(\"station\").drop(\"component\", axis=1) for v in IM_DATA.values()\n",
    "        for df in v.values()\n",
    "        if df is not None and not df.empty\n",
    "    ]\n",
    "\n",
    "    ref_index = all_dfs[0].index\n",
    "    ref_columns = all_dfs[0].columns\n",
    "    all_dfs_aligned = [\n",
    "        df.reindex(index=ref_index, columns=ref_columns).astype(np.float64)\n",
    "        for df in all_dfs\n",
    "    ]\n",
    "\n",
    "    array_stack = np.stack([df.to_numpy() for df in all_dfs_aligned], axis=0)\n",
    "    median_array = np.nanmedian(array_stack, axis=0)\n",
    "    \n",
    "    median_df = pd.DataFrame(median_array, index=ref_index, columns=ref_columns)\n",
    "    median_df.insert(0, \"station\", median_df.index)  # Insert 'station' as the first column\n",
    "    median_df.insert(1, \"component\", Comp)         # Add 'component' column with Comp value\n",
    "    # write the median DataFrame to the CSV file\n",
    "    median_im_file = os.path.join(combined_cs_files_path, cur_fault, 'IM', f\"{cur_fault}.csv\")\n",
    "    median_df.to_csv(median_im_file, index=False)     \n",
    "    \n",
    "    ###############################################################################\n",
    "    ########################### working on Source files ###########################\n",
    "    ###############################################################################     \n",
    "    \n",
    "    os.makedirs(os.path.join(combined_cs_files_path, cur_fault, 'Source'))\n",
    "    file_counter = 0\n",
    "\n",
    "    for v in cur_fault_versions:\n",
    "        # check if the source and IM files are one-to-one\n",
    "        fault_version_source = [rel for rel in Path(Path(vers_path[v]) / cur_fault / \"Source\").rglob(\"*REL*.csv\") if \".pertb.\" not in rel.name]\n",
    "        fault_version_source.sort()\n",
    "        realizations_im_files = [r.stem for r in fault_version_realizations]\n",
    "        realizations_sr_files = [r.stem for r in fault_version_source]\n",
    "        \n",
    "        only_in_im = set(realizations_im_files) - set(realizations_sr_files)\n",
    "        only_in_sr = set(realizations_sr_files) - set(realizations_im_files)\n",
    "        \n",
    "        if len(only_in_im) != 0:\n",
    "            for item in only_in_im:\n",
    "                msg = f\"Warning(IM_without_Source): Realization {item} in {v}/{cur_fault} does not have source file!\"\n",
    "                log_lines.append(msg)\n",
    "        \n",
    "        if len(only_in_sr) != 0:\n",
    "            for item in only_in_sr:\n",
    "                msg = f\"Warning(Source_without_IM): Source {item} in {v}/{cur_fault} does not have IM file!\"\n",
    "                log_lines.append(msg)\n",
    "        \n",
    "        # check if the info and IM files are one-to-one\n",
    "        fault_version_info = [rel for rel in Path(Path(vers_path[v]) / cur_fault / \"Source\").rglob(\"*REL*.info\")]\n",
    "        fault_version_info.sort()\n",
    "        realizations_info_files = [r.stem for r in fault_version_info]\n",
    "\n",
    "        only_in_im_vs_info = set(realizations_im_files) - set(realizations_info_files)\n",
    "        only_in_info_vs_im = set(realizations_info_files) - set(realizations_im_files)\n",
    "\n",
    "        if len(only_in_im_vs_info) != 0:\n",
    "            for item in only_in_im_vs_info:\n",
    "                msg = f\"Warning(IM_without_Info): Realization {item} in {v}/{cur_fault} does not have info file!\"\n",
    "                log_lines.append(msg)\n",
    "\n",
    "        if len(only_in_info_vs_im) != 0:\n",
    "            for item in only_in_info_vs_im:\n",
    "                msg = f\"Warning(Info_without_IM): Info {item} in {v}/{cur_fault} does not have IM file!\"\n",
    "                log_lines.append(msg)\n",
    "\n",
    "        # check if the info and IM files are one-to-one\n",
    "        fault_version_pertb = [rel for rel in Path(Path(vers_path[v]) / cur_fault / \"Source\").rglob(\"*REL*.pertb.csv\")]\n",
    "        fault_version_pertb.sort()\n",
    "        realizations_pertb_files = [r.stem.removesuffix('.pertb') for r in fault_version_pertb]\n",
    "\n",
    "        only_in_im_vs_pertb = set(realizations_im_files) - set(realizations_pertb_files)\n",
    "        only_in_pertb_vs_im = set(realizations_pertb_files) - set(realizations_im_files)\n",
    "\n",
    "        if len(only_in_im_vs_pertb) != 0:\n",
    "            for item in only_in_im_vs_pertb:\n",
    "                msg = f\"Warning(IM_without_Pertb): Realization {item} in {v}/{cur_fault} does not have pertb file!\"\n",
    "                log_lines.append(msg)\n",
    "\n",
    "        if len(only_in_pertb_vs_im) != 0:\n",
    "            for item in only_in_pertb_vs_im:\n",
    "                msg = f\"Warning(Pertb_without_IM): Pertb {item} in {v}/{cur_fault} does not have IM file!\"\n",
    "                log_lines.append(msg)\n",
    "                \n",
    "        # check if median source file exists\n",
    "        if not Path(Path(vers_path[v]) / cur_fault / \"Source\" / f\"{cur_fault}.csv\").exists():\n",
    "            msg = f\"Warning(Missing_source_mediam_file): Source median file does not exist for {v}/{cur_fault} !\"\n",
    "            log_lines.append(msg)\n",
    "        \n",
    "        # check if vm_params.yaml file exists\n",
    "        if not Path(Path(vers_path[v]) / cur_fault / \"Source\" / \"vm_params.yaml\").exists():\n",
    "            msg = f\"Warning(Missing_vm_params_file): vm_params.yaml file does not exist for {v}/{cur_fault} !\"\n",
    "            log_lines.append(msg)\n",
    "            \n",
    "        # check if nzvm.cfg file exists\n",
    "        if not Path(Path(vers_path[v]) / cur_fault / \"Source\" / \"nzvm.cfg\").exists():\n",
    "            msg = f\"Warning(Missing_nzvm_config_file): nzvm.cfg file does not exist for {v}/{cur_fault} !\"\n",
    "            log_lines.append(msg)\n",
    "        \n",
    "\n",
    "        src_folder = Path(vers_path[v]) / cur_fault / \"Source\"\n",
    "        dst_folder = Path(combined_cs_files_path) / cur_fault / \"Source\"\n",
    "        # copy source file   \n",
    "        for item in realizations_sr_files:\n",
    "            src = src_folder / f\"{item}.csv\"\n",
    "            dst = dst_folder / f\"{item}.csv\"\n",
    "            if src.exists():\n",
    "                shutil.copy2(src, dst)\n",
    "                \n",
    "        # copy pertb file    \n",
    "        for item in realizations_pertb_files:\n",
    "            original_path = os.path.join(vers_path[v], cur_fault, \"Source\", f\"{item}.pertb.csv\")\n",
    "            destination_path = os.path.join(combined_cs_files_path, cur_fault, \"Source\", f\"{item}.pertb.csv\")\n",
    "            shutil.copy2(original_path, destination_path)\n",
    "        \n",
    "        # copy info file    \n",
    "        for item in realizations_info_files:\n",
    "            src = src_folder / f\"{item}.info\"\n",
    "            dst = dst_folder / f\"{item}.info\"\n",
    "            if src.exists():\n",
    "                shutil.copy2(src, dst)\n",
    "              \n",
    "        # copy median source file if exists\n",
    "        src = Path(vers_path[v]) / cur_fault / \"Source\" / f\"{cur_fault}.csv\"\n",
    "        dst = Path(combined_cs_files_path) / cur_fault / \"Source\" / f\"{cur_fault}.csv\"\n",
    "        if src.exists():\n",
    "            shutil.copy2(src, dst)\n",
    "            \n",
    "        # copy median info file if exists\n",
    "        src = Path(vers_path[v]) / cur_fault / \"Source\" / f\"{cur_fault}.info\"\n",
    "        dst = Path(combined_cs_files_path) / cur_fault / \"Source\" / f\"{cur_fault}.info\"\n",
    "        if src.exists():\n",
    "            shutil.copy2(src, dst)\n",
    "        \n",
    "        # check if vm_params.yaml file exists\n",
    "        src = Path(vers_path[v]) / cur_fault / \"Source\" / \"vm_params.yaml\"\n",
    "        dst = Path(combined_cs_files_path) / cur_fault / \"Source\" / f\"vm_params_from_cs100_{v}.yaml\"\n",
    "        if src.exists():\n",
    "            shutil.copy2(src, dst)\n",
    "            \n",
    "        # check if nzvm.cfg file exists\n",
    "        src = Path(vers_path[v]) / cur_fault / \"Source\" / \"nzvm.cfg\"\n",
    "        dst = Path(combined_cs_files_path) / cur_fault / \"Source\" / f\"nzvm_from_cs100_{v}.cfg\"\n",
    "        if src.exists():\n",
    "            shutil.copy2(src, dst)\n",
    "             \n",
    "    \n",
    "with open(log_file, 'w') as f:\n",
    "    f.write(\"\\n\".join(log_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stations and IMs data saved to /mnt/hypo_data/mab419/cs100/Cybershake_Data/combined_cs100/version_station_im_summary.csv\n",
      "Note written to /mnt/hypo_data/mab419/cs100/Cybershake_Data/combined_cs100/note.txt\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV\n",
    "station_im_summary_file_path = os.path.join(combined_cs_files_path, \"version_station_im_summary.csv\")\n",
    "summary_data.to_csv(station_im_summary_file_path)\n",
    "print(f\"Stations and IMs data saved to {station_im_summary_file_path}\")\n",
    "\n",
    "# Write Note\n",
    "Current_Date_and_Time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "note_file_path = os.path.join(combined_cs_files_path, 'note.txt')\n",
    "[f\"{v}, \" for v in versions]\n",
    "note_content = (\n",
    "    f\"- This folder contains combined IM (Intensity Measure) data from the realizations of the following versions:\\n\"\n",
    "    f\"[{', '.join(f'\\\"{v}\\\"' for v in versions)}]\\n\"\n",
    "    f\"- Date of generation: {Current_Date_and_Time}\\n\"\n",
    "    f\"- Key Notes:\\n\"\n",
    "    f\"    1. For each fault (source), only the common stations and IM types between the realizations (even differne tversions) have been retained.\\n\"\n",
    "    f\"    2. Median IM values for each station and IM type have been computed and saved as a separate CSV file.\\n\"\n",
    "    f\"    3. Source files have been copied:\\n\"\n",
    "    f\"       - 'nzvm.cfg' and 'vm_params.yaml' from each version are copied with a '_from_cs100_version' suffix.\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "# Write to file\n",
    "with open(note_file_path, \"w\") as file:\n",
    "    file.write(note_content)\n",
    "\n",
    "print(f\"Note written to {note_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "morteza",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
